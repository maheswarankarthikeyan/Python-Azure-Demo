{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Module 12: Agent Evaluation & Observability\n",
    "\n",
    "**AI Agent Architectures Workshop - Day 2**\n",
    "\n",
    "This notebook covers:\n",
    "- Measuring tool call accuracy\n",
    "- Measuring goal accuracy\n",
    "- Azure Monitor integration\n",
    "- Application Insights for agent workflows\n",
    "- Safe interaction design\n",
    "\n",
    "**Prerequisites:** Run `00_setup.ipynb` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv opencensus-ext-azure --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# =============================================================================\n",
    "# GOOGLE COLAB SETUP - Add these secrets (click ðŸ”‘ icon):\n",
    "#   - AZURE_OPENAI_KEY: Your API key\n",
    "#   - AZURE_OPENAI_ENDPOINT: https://xxx.openai.azure.com/\n",
    "#   - AZURE_OPENAI_DEPLOYMENT: Your model deployment name\n",
    "# =============================================================================\n",
    "\n",
    "DEMO_MODE = False\n",
    "client = None\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    AZURE_OPENAI_KEY = userdata.get('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_ENDPOINT = userdata.get('AZURE_OPENAI_ENDPOINT')\n",
    "    try:\n",
    "        MODEL_NAME = userdata.get('AZURE_OPENAI_DEPLOYMENT')\n",
    "    except:\n",
    "        pass\n",
    "    if AZURE_OPENAI_KEY and AZURE_OPENAI_ENDPOINT:\n",
    "        if not AZURE_OPENAI_ENDPOINT.startswith('http'):\n",
    "            AZURE_OPENAI_ENDPOINT = 'https://' + AZURE_OPENAI_ENDPOINT\n",
    "        print(f\"âœ… Credentials loaded. Model: {MODEL_NAME}\")\n",
    "    else:\n",
    "        raise ValueError(\"Missing\")\n",
    "except Exception:\n",
    "    print(\"âš ï¸ Running in DEMO MODE\")\n",
    "    DEMO_MODE = True\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    from openai import AzureOpenAI\n",
    "    client = AzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_KEY,\n",
    "        api_version=\"2024-06-01\",\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    "    )\n",
    "    print(\"âœ… Client ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tool Call Accuracy Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ToolCallEvaluation:\n",
    "    \"\"\"Evaluation result for a single tool call\"\"\"\n",
    "    query: str\n",
    "    expected_tool: str\n",
    "    actual_tool: Optional[str]\n",
    "    expected_params: Dict\n",
    "    actual_params: Optional[Dict]\n",
    "    tool_correct: bool\n",
    "    params_correct: bool\n",
    "    latency_ms: float\n",
    "\n",
    "class ToolCallEvaluator:\n",
    "    \"\"\"Evaluate agent tool calling accuracy\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_account_balance\",\n",
    "                    \"description\": \"Get the current balance of a bank account\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"account_id\": {\"type\": \"string\", \"description\": \"The account ID\"}\n",
    "                        },\n",
    "                        \"required\": [\"account_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"transfer_funds\",\n",
    "                    \"description\": \"Transfer money between accounts\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"from_account\": {\"type\": \"string\"},\n",
    "                            \"to_account\": {\"type\": \"string\"},\n",
    "                            \"amount\": {\"type\": \"number\"}\n",
    "                        },\n",
    "                        \"required\": [\"from_account\", \"to_account\", \"amount\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_transaction_history\",\n",
    "                    \"description\": \"Get recent transactions for an account\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"account_id\": {\"type\": \"string\"},\n",
    "                            \"days\": {\"type\": \"integer\", \"default\": 30}\n",
    "                        },\n",
    "                        \"required\": [\"account_id\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        self.evaluations: List[ToolCallEvaluation] = []\n",
    "    \n",
    "    def evaluate(self, query: str, expected_tool: str, expected_params: Dict) -> ToolCallEvaluation:\n",
    "        \"\"\"Evaluate a single tool call\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": query}],\n",
    "            tools=self.tools,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Extract actual tool call\n",
    "        actual_tool = None\n",
    "        actual_params = None\n",
    "        \n",
    "        if response.choices[0].message.tool_calls:\n",
    "            tool_call = response.choices[0].message.tool_calls[0]\n",
    "            actual_tool = tool_call.function.name\n",
    "            actual_params = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # Evaluate correctness\n",
    "        tool_correct = actual_tool == expected_tool\n",
    "        params_correct = self._compare_params(expected_params, actual_params)\n",
    "        \n",
    "        evaluation = ToolCallEvaluation(\n",
    "            query=query,\n",
    "            expected_tool=expected_tool,\n",
    "            actual_tool=actual_tool,\n",
    "            expected_params=expected_params,\n",
    "            actual_params=actual_params,\n",
    "            tool_correct=tool_correct,\n",
    "            params_correct=params_correct,\n",
    "            latency_ms=latency_ms\n",
    "        )\n",
    "        \n",
    "        self.evaluations.append(evaluation)\n",
    "        return evaluation\n",
    "    \n",
    "    def _compare_params(self, expected: Dict, actual: Optional[Dict]) -> bool:\n",
    "        \"\"\"Compare expected and actual parameters\"\"\"\n",
    "        if actual is None:\n",
    "            return False\n",
    "        for key, value in expected.items():\n",
    "            if key not in actual or actual[key] != value:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        if not self.evaluations:\n",
    "            return {}\n",
    "        \n",
    "        total = len(self.evaluations)\n",
    "        tool_correct = sum(1 for e in self.evaluations if e.tool_correct)\n",
    "        params_correct = sum(1 for e in self.evaluations if e.params_correct)\n",
    "        avg_latency = sum(e.latency_ms for e in self.evaluations) / total\n",
    "        \n",
    "        return {\n",
    "            \"total_evaluations\": total,\n",
    "            \"tool_accuracy\": tool_correct / total,\n",
    "            \"param_accuracy\": params_correct / total,\n",
    "            \"avg_latency_ms\": avg_latency\n",
    "        }\n",
    "\n",
    "print(\"âœ… Tool call evaluator defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tool call evaluation\n",
    "evaluator = ToolCallEvaluator()\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What's the balance in account A123?\",\n",
    "        \"expected_tool\": \"get_account_balance\",\n",
    "        \"expected_params\": {\"account_id\": \"A123\"}\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Transfer $500 from account A123 to account B456\",\n",
    "        \"expected_tool\": \"transfer_funds\",\n",
    "        \"expected_params\": {\"from_account\": \"A123\", \"to_account\": \"B456\", \"amount\": 500}\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Show me the last 7 days of transactions for account A123\",\n",
    "        \"expected_tool\": \"get_transaction_history\",\n",
    "        \"expected_params\": {\"account_id\": \"A123\", \"days\": 7}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== Tool Call Evaluation ===\")\n",
    "for tc in test_cases:\n",
    "    result = evaluator.evaluate(tc[\"query\"], tc[\"expected_tool\"], tc[\"expected_params\"])\n",
    "    status = \"âœ…\" if result.tool_correct and result.params_correct else \"âŒ\"\n",
    "    print(f\"{status} Query: {tc['query'][:40]}...\")\n",
    "    print(f\"   Expected: {tc['expected_tool']} | Actual: {result.actual_tool}\")\n",
    "    print(f\"   Tool correct: {result.tool_correct} | Params correct: {result.params_correct}\")\n",
    "\n",
    "metrics = evaluator.get_metrics()\n",
    "print(f\"\\nðŸ“Š Metrics:\")\n",
    "print(f\"   Tool Accuracy: {metrics['tool_accuracy']:.1%}\")\n",
    "print(f\"   Param Accuracy: {metrics['param_accuracy']:.1%}\")\n",
    "print(f\"   Avg Latency: {metrics['avg_latency_ms']:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Goal Accuracy Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GoalEvaluation:\n",
    "    \"\"\"Evaluation of whether agent achieved the goal\"\"\"\n",
    "    goal: str\n",
    "    agent_response: str\n",
    "    goal_achieved: bool\n",
    "    reasoning: str\n",
    "    score: float  # 0-1\n",
    "\n",
    "class GoalEvaluator:\n",
    "    \"\"\"Evaluate whether agent responses achieve stated goals\"\"\"\n",
    "    \n",
    "    def evaluate(self, goal: str, agent_response: str) -> GoalEvaluation:\n",
    "        \"\"\"Use LLM-as-judge to evaluate goal achievement\"\"\"\n",
    "        \n",
    "        eval_prompt = f\"\"\"Evaluate whether the agent's response achieves the stated goal.\n",
    "\n",
    "GOAL: {goal}\n",
    "\n",
    "AGENT RESPONSE: {agent_response}\n",
    "\n",
    "Evaluate on these criteria:\n",
    "1. Did the agent address the goal directly?\n",
    "2. Was the response accurate and helpful?\n",
    "3. Were there any errors or hallucinations?\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "    \"goal_achieved\": true/false,\n",
    "    \"score\": 0.0-1.0,\n",
    "    \"reasoning\": \"explanation\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        return GoalEvaluation(\n",
    "            goal=goal,\n",
    "            agent_response=agent_response,\n",
    "            goal_achieved=result[\"goal_achieved\"],\n",
    "            reasoning=result[\"reasoning\"],\n",
    "            score=result[\"score\"]\n",
    "        )\n",
    "\n",
    "# Test goal evaluation\n",
    "goal_evaluator = GoalEvaluator()\n",
    "\n",
    "test_goals = [\n",
    "    {\n",
    "        \"goal\": \"Explain how to dispute a transaction\",\n",
    "        \"response\": \"To dispute a transaction, log into your online banking, go to the transaction, and click 'Dispute'. You have 60 days from the statement date to file a dispute.\"\n",
    "    },\n",
    "    {\n",
    "        \"goal\": \"Provide the current mortgage rate\",\n",
    "        \"response\": \"I'm sorry, I don't have access to current rates. Please visit our website or call customer service.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== Goal Achievement Evaluation ===\")\n",
    "for test in test_goals:\n",
    "    result = goal_evaluator.evaluate(test[\"goal\"], test[\"response\"])\n",
    "    status = \"âœ…\" if result.goal_achieved else \"âŒ\"\n",
    "    print(f\"\\n{status} Goal: {test['goal']}\")\n",
    "    print(f\"   Score: {result.score:.1%}\")\n",
    "    print(f\"   Reasoning: {result.reasoning[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Observability with Azure Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTelemetry:\n",
    "    \"\"\"Collect and report agent telemetry (simulated Azure Monitor)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = []\n",
    "        self.traces = []\n",
    "        self.events = []\n",
    "    \n",
    "    def track_metric(self, name: str, value: float, properties: Dict = None):\n",
    "        \"\"\"Track a custom metric\"\"\"\n",
    "        self.metrics.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"name\": name,\n",
    "            \"value\": value,\n",
    "            \"properties\": properties or {}\n",
    "        })\n",
    "    \n",
    "    def track_trace(self, message: str, severity: str = \"INFO\", properties: Dict = None):\n",
    "        \"\"\"Track a trace message\"\"\"\n",
    "        self.traces.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"message\": message,\n",
    "            \"severity\": severity,\n",
    "            \"properties\": properties or {}\n",
    "        })\n",
    "    \n",
    "    def track_event(self, name: str, properties: Dict = None):\n",
    "        \"\"\"Track a custom event\"\"\"\n",
    "        self.events.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"name\": name,\n",
    "            \"properties\": properties or {}\n",
    "        })\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get telemetry summary\"\"\"\n",
    "        return {\n",
    "            \"total_metrics\": len(self.metrics),\n",
    "            \"total_traces\": len(self.traces),\n",
    "            \"total_events\": len(self.events),\n",
    "            \"recent_metrics\": self.metrics[-5:] if self.metrics else []\n",
    "        }\n",
    "\n",
    "# Instrumented agent\n",
    "class InstrumentedAgent:\n",
    "    \"\"\"Agent with full observability\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.telemetry = AgentTelemetry()\n",
    "    \n",
    "    def invoke(self, query: str) -> str:\n",
    "        \"\"\"Invoke agent with telemetry\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Track request event\n",
    "        self.telemetry.track_event(\"agent_request\", {\"query_length\": len(query)})\n",
    "        self.telemetry.track_trace(f\"Processing query: {query[:50]}...\", \"INFO\")\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": query}]\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Track success metrics\n",
    "            latency = (time.time() - start_time) * 1000\n",
    "            tokens = response.usage.total_tokens\n",
    "            \n",
    "            self.telemetry.track_metric(\"latency_ms\", latency)\n",
    "            self.telemetry.track_metric(\"tokens_used\", tokens)\n",
    "            self.telemetry.track_event(\"agent_success\", {\n",
    "                \"latency_ms\": latency,\n",
    "                \"tokens\": tokens\n",
    "            })\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Track error\n",
    "            self.telemetry.track_trace(f\"Error: {str(e)}\", \"ERROR\")\n",
    "            self.telemetry.track_event(\"agent_error\", {\"error\": str(e)})\n",
    "            raise\n",
    "\n",
    "# Test instrumented agent\n",
    "agent = InstrumentedAgent()\n",
    "\n",
    "queries = [\n",
    "    \"What is the interest rate on savings accounts?\",\n",
    "    \"How do I open a new checking account?\",\n",
    "    \"What are the requirements for a business loan?\"\n",
    "]\n",
    "\n",
    "print(\"=== Instrumented Agent Execution ===\")\n",
    "for query in queries:\n",
    "    response = agent.invoke(query)\n",
    "    print(f\"âœ… Processed: {query[:40]}...\")\n",
    "\n",
    "summary = agent.telemetry.get_summary()\n",
    "print(f\"\\nðŸ“Š Telemetry Summary:\")\n",
    "print(f\"   Metrics: {summary['total_metrics']}\")\n",
    "print(f\"   Traces: {summary['total_traces']}\")\n",
    "print(f\"   Events: {summary['total_events']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Safe Interaction Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafetyGuard:\n",
    "    \"\"\"Safety checks for agent interactions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.blocked_patterns = [\n",
    "            \"password\", \"ssn\", \"social security\",\n",
    "            \"credit card number\", \"pin\"\n",
    "        ]\n",
    "        self.high_risk_actions = [\n",
    "            \"transfer\", \"wire\", \"close account\", \"change password\"\n",
    "        ]\n",
    "    \n",
    "    def check_input(self, user_input: str) -> Dict:\n",
    "        \"\"\"Check user input for safety issues\"\"\"\n",
    "        input_lower = user_input.lower()\n",
    "        \n",
    "        # Check for PII requests\n",
    "        pii_detected = any(p in input_lower for p in self.blocked_patterns)\n",
    "        \n",
    "        # Check for high-risk actions\n",
    "        high_risk = any(a in input_lower for a in self.high_risk_actions)\n",
    "        \n",
    "        return {\n",
    "            \"safe\": not pii_detected,\n",
    "            \"requires_confirmation\": high_risk,\n",
    "            \"pii_detected\": pii_detected,\n",
    "            \"high_risk_action\": high_risk\n",
    "        }\n",
    "    \n",
    "    def check_output(self, agent_output: str) -> Dict:\n",
    "        \"\"\"Check agent output for safety issues\"\"\"\n",
    "        output_lower = agent_output.lower()\n",
    "        \n",
    "        # Check for potential PII leakage\n",
    "        pii_patterns = [\n",
    "            r'\\d{3}-\\d{2}-\\d{4}',  # SSN\n",
    "            r'\\d{16}',  # Credit card\n",
    "            r'\\d{4}-\\d{4}-\\d{4}-\\d{4}'  # Credit card with dashes\n",
    "        ]\n",
    "        \n",
    "        import re\n",
    "        pii_found = any(re.search(p, agent_output) for p in pii_patterns)\n",
    "        \n",
    "        return {\n",
    "            \"safe\": not pii_found,\n",
    "            \"pii_leaked\": pii_found\n",
    "        }\n",
    "\n",
    "# Test safety guard\n",
    "guard = SafetyGuard()\n",
    "\n",
    "test_inputs = [\n",
    "    \"What is my account balance?\",\n",
    "    \"What is my password?\",\n",
    "    \"Transfer $5000 to account 12345\",\n",
    "    \"Tell me my social security number\"\n",
    "]\n",
    "\n",
    "print(\"=== Safety Guard Checks ===\")\n",
    "for inp in test_inputs:\n",
    "    result = guard.check_input(inp)\n",
    "    status = \"âœ…\" if result[\"safe\"] else \"ðŸš«\"\n",
    "    confirm = \"âš ï¸ Needs confirmation\" if result[\"requires_confirmation\"] else \"\"\n",
    "    print(f\"{status} {inp[:40]}... {confirm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Agent Evaluation Metrics:**\n",
    "\n",
    "| Metric | Description | Target |\n",
    "|--------|-------------|--------|\n",
    "| Tool Call Accuracy | Correct tool selected | >95% |\n",
    "| Parameter Accuracy | Correct parameters extracted | >90% |\n",
    "| Goal Achievement | Task completed successfully | >85% |\n",
    "| Latency P95 | 95th percentile response time | <3s |\n",
    "\n",
    "**Azure Monitor Integration:**\n",
    "- Application Insights for traces and metrics\n",
    "- Custom events for agent actions\n",
    "- Alerts for error rates and latency\n",
    "\n",
    "**Safety Checklist:**\n",
    "- Input validation for PII requests\n",
    "- Output scanning for PII leakage\n",
    "- Confirmation for high-risk actions\n",
    "- Audit logging for compliance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
