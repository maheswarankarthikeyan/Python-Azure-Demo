{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Lab 12: Model Selection & Evaluation\n",
    "## Module 12 - Trade-offs: Latency, Cost, Accuracy\n",
    "\n",
    "**Duration:** 20 minutes\n",
    "\n",
    "**Objectives:**\n",
    "- Compare model performance for banking tasks\n",
    "- Evaluate with appropriate metrics\n",
    "- Make cost-effective model choices\n",
    "\n",
    "**Banking Scenario:** Select optimal model for document classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# GOOGLE COLAB SETUP - Add these secrets (click ðŸ”‘ icon):\n",
    "#   - AZURE_OPENAI_KEY: Your API key\n",
    "#   - AZURE_OPENAI_ENDPOINT: https://xxx.openai.azure.com/\n",
    "#   - AZURE_OPENAI_DEPLOYMENT: Your model deployment name\n",
    "# =============================================================================\n",
    "\n",
    "DEMO_MODE = False\n",
    "client = None\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    AZURE_OPENAI_KEY = userdata.get('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_ENDPOINT = userdata.get('AZURE_OPENAI_ENDPOINT')\n",
    "    try:\n",
    "        MODEL_NAME = userdata.get('AZURE_OPENAI_DEPLOYMENT')\n",
    "    except:\n",
    "        pass\n",
    "    if AZURE_OPENAI_KEY and AZURE_OPENAI_ENDPOINT:\n",
    "        if not AZURE_OPENAI_ENDPOINT.startswith('http'):\n",
    "            AZURE_OPENAI_ENDPOINT = 'https://' + AZURE_OPENAI_ENDPOINT\n",
    "        print(f\"âœ… Credentials loaded. Model: {MODEL_NAME}\")\n",
    "    else:\n",
    "        raise ValueError(\"Missing\")\n",
    "except Exception:\n",
    "    print(\"âš ï¸ Running in DEMO MODE\")\n",
    "    DEMO_MODE = True\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    from openai import AzureOpenAI\n",
    "    client = AzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_KEY,\n",
    "        api_version=\"2024-06-01\",\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    "    )\n",
    "    print(\"âœ… Client ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n    \"gpt-4o\": {\"cost_input\": 5.0, \"cost_output\": 15.0},\n    \"gpt-4o-mini\": {\"cost_input\": 0.15, \"cost_output\": 0.60}\n}\n\ndef benchmark_model(model: str, prompt: str, expected: str) -> dict:\n    \"\"\"Benchmark a model on a single task\"\"\"\n    start = time.time()\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=100\n    )\n    \n    latency = time.time() - start\n    output = response.choices[0].message.content\n    tokens = response.usage.total_tokens\n    \n    # Calculate cost\n    pricing = MODELS[model]\n    cost = (response.usage.prompt_tokens * pricing[\"cost_input\"] + \n            response.usage.completion_tokens * pricing[\"cost_output\"]) / 1_000_000\n    \n    # Simple accuracy check\n    correct = expected.lower() in output.lower()\n    \n    return {\n        \"model\": model,\n        \"latency\": latency,\n        \"tokens\": tokens,\n        \"cost\": cost,\n        \"correct\": correct,\n        \"output\": output[:100]\n    }\n\nprint(\"âœ… Benchmark function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for banking document classification\ntest_cases = [\n    {\"prompt\": \"Classify this document: 'Application for $250,000 home loan at 6.5% for 30 years'\", \"expected\": \"mortgage\"},\n    {\"prompt\": \"Classify this document: 'Request to dispute charge of $45.99 from unknown merchant'\", \"expected\": \"dispute\"},\n    {\"prompt\": \"Classify this document: 'Inquiry about current CD rates and terms'\", \"expected\": \"inquiry\"}\n]\n\nresults = []\nfor model in MODELS.keys():\n    print(f\"\\nTesting {model}...\")\n    for test in test_cases:\n        result = benchmark_model(model, test[\"prompt\"], test[\"expected\"])\n        results.append(result)\n        print(f\"  âœ“ {test['expected']}: {result['latency']:.2f}s, ${result['cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results by model\nfrom collections import defaultdict\n\nstats = defaultdict(lambda: {\"latency\": [], \"cost\": [], \"correct\": 0, \"total\": 0})\n\nfor r in results:\n    model = r[\"model\"]\n    stats[model][\"latency\"].append(r[\"latency\"])\n    stats[model][\"cost\"].append(r[\"cost\"])\n    stats[model][\"correct\"] += 1 if r[\"correct\"] else 0\n    stats[model][\"total\"] += 1\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODEL COMPARISON RESULTS\")\nprint(\"=\"*60)\nprint(f\"{'Model':<15} {'Avg Latency':<12} {'Avg Cost':<12} {'Accuracy':<10}\")\nprint(\"-\"*60)\n\nfor model, data in stats.items():\n    avg_latency = sum(data[\"latency\"]) / len(data[\"latency\"])\n    avg_cost = sum(data[\"cost\"]) / len(data[\"cost\"])\n    accuracy = data[\"correct\"] / data[\"total\"]\n    print(f\"{model:<15} {avg_latency:.2f}s{'':<6} ${avg_cost:.6f}{'':<4} {accuracy:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cost Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project costs at scale\nDAILY_REQUESTS = 10000\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COST PROJECTION (10,000 daily requests)\")\nprint(\"=\"*60)\n\nfor model, data in stats.items():\n    avg_cost = sum(data[\"cost\"]) / len(data[\"cost\"])\n    daily = avg_cost * DAILY_REQUESTS\n    monthly = daily * 30\n    print(f\"{model}: ${daily:.2f}/day, ${monthly:.2f}/month\")\n\nprint(\"\\nðŸ’¡ Recommendation: Use GPT-4o-mini for classification tasks\")\nprint(\"   Savings: ~97% with comparable accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Lab 12 Complete!\n",
    "\n",
    "**Key Takeaways:**\n",
    "- GPT-4o-mini is 97% cheaper with similar accuracy for simple tasks\n",
    "- Always benchmark before choosing a model\n",
    "- Consider latency requirements for real-time applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
