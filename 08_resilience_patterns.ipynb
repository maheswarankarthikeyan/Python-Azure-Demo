{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ Lab 8: Resilience Patterns\n",
    "## Module 8 - Error Handling, Fallbacks, Circuit Breakers\n",
    "\n",
    "**Duration:** 15 minutes\n",
    "\n",
    "**Objectives:**\n",
    "- Implement retry with exponential backoff\n",
    "- Build model fallback chains\n",
    "- Add confidence-based routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai tenacity -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "DEMO_MODE = False\n",
    "client = None\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    AZURE_OPENAI_KEY = userdata.get('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_ENDPOINT = userdata.get('AZURE_OPENAI_ENDPOINT')\n",
    "    try: MODEL_NAME = userdata.get('AZURE_OPENAI_DEPLOYMENT')\n",
    "    except: pass\n",
    "    if AZURE_OPENAI_KEY and AZURE_OPENAI_ENDPOINT:\n",
    "        if not AZURE_OPENAI_ENDPOINT.startswith('http'):\n",
    "            AZURE_OPENAI_ENDPOINT = 'https://' + AZURE_OPENAI_ENDPOINT\n",
    "        print(f\"âœ… Loaded. Model: {MODEL_NAME}\")\n",
    "    else: raise ValueError()\n",
    "except: print(\"âš ï¸ DEMO MODE\"); DEMO_MODE = True\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    from openai import AzureOpenAI\n",
    "    client = AzureOpenAI(api_key=AZURE_OPENAI_KEY, api_version=\"2024-06-01\", azure_endpoint=AZURE_OPENAI_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Retry with Exponential Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=30))\n",
    "def call_llm_with_retry(messages):\n",
    "    if DEMO_MODE or not client:\n",
    "        return \"Demo response [DEMO]\"\n",
    "    response = client.chat.completions.create(model=MODEL_NAME, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "result = call_llm_with_retry([{\"role\": \"user\", \"content\": \"What is 2+2?\"}])\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Fallback Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFallbackChain:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.models = [MODEL_NAME, \"gpt-4o-mini\"] if not DEMO_MODE else []\n",
    "        self.fallback = \"I'm experiencing issues. Please try again or call 1-800-BANK.\"\n",
    "    \n",
    "    def invoke(self, messages):\n",
    "        if DEMO_MODE or not self.client:\n",
    "            return {\"content\": \"Demo fallback response [DEMO]\", \"model_used\": \"demo\", \"fallback\": True}\n",
    "        \n",
    "        for model in self.models:\n",
    "            try:\n",
    "                print(f\"ðŸ”„ Trying {model}...\")\n",
    "                response = self.client.chat.completions.create(model=model, messages=messages)\n",
    "                return {\"content\": response.choices[0].message.content, \"model_used\": model, \"fallback\": model != self.models[0]}\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ {model} failed: {e}\")\n",
    "        \n",
    "        return {\"content\": self.fallback, \"model_used\": \"cached\", \"fallback\": True}\n",
    "\n",
    "chain = ModelFallbackChain(client)\n",
    "result = chain.invoke([{\"role\": \"user\", \"content\": \"What's my balance?\"}])\n",
    "print(f\"\\nâœ… Response from: {result['model_used']}\")\n",
    "print(f\"Content: {result['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Circuit Breaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitBreaker:\n",
    "    def __init__(self, threshold=3, timeout=60):\n",
    "        self.threshold = threshold\n",
    "        self.timeout = timeout\n",
    "        self.failures = 0\n",
    "        self.state = \"CLOSED\"\n",
    "        self.last_failure = None\n",
    "    \n",
    "    def can_execute(self):\n",
    "        if self.state == \"CLOSED\": return True\n",
    "        if self.state == \"OPEN\" and self.last_failure:\n",
    "            if (datetime.now() - self.last_failure).seconds >= self.timeout:\n",
    "                self.state = \"HALF_OPEN\"\n",
    "                return True\n",
    "        return self.state == \"HALF_OPEN\"\n",
    "    \n",
    "    def record_success(self):\n",
    "        if self.state == \"HALF_OPEN\": self.state = \"CLOSED\"; self.failures = 0\n",
    "    \n",
    "    def record_failure(self):\n",
    "        self.failures += 1\n",
    "        self.last_failure = datetime.now()\n",
    "        if self.failures >= self.threshold: self.state = \"OPEN\"; print(\"ðŸš¨ Circuit OPEN\")\n",
    "\n",
    "cb = CircuitBreaker(threshold=3)\n",
    "print(\"Simulating failures...\")\n",
    "for i in range(5):\n",
    "    if cb.can_execute():\n",
    "        print(f\"  Request {i+1}: Executing (fail)\")\n",
    "        cb.record_failure()\n",
    "    else:\n",
    "        print(f\"  Request {i+1}: BLOCKED\")\n",
    "\n",
    "print(f\"\\nState: {cb.state}, Failures: {cb.failures}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Confidence Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfidenceAction(Enum):\n",
    "    AUTO_EXECUTE = \"auto\"\n",
    "    CONFIRM = \"confirm\"\n",
    "    ESCALATE = \"escalate\"\n",
    "\n",
    "def analyze_with_confidence(query):\n",
    "    if DEMO_MODE or not client:\n",
    "        # Demo confidence based on query type\n",
    "        if \"hours\" in query.lower() or \"balance\" in query.lower():\n",
    "            return {\"response\": \"Branch hours: 9am-5pm [DEMO]\", \"confidence\": 0.95, \"action\": ConfidenceAction.AUTO_EXECUTE}\n",
    "        elif \"invest\" in query.lower() or \"should i\" in query.lower():\n",
    "            return {\"response\": \"Please consult advisor [DEMO]\", \"confidence\": 0.4, \"action\": ConfidenceAction.ESCALATE}\n",
    "        return {\"response\": \"General response [DEMO]\", \"confidence\": 0.75, \"action\": ConfidenceAction.CONFIRM}\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"Respond with JSON: {\\\"response\\\": \\\"...\\\", \\\"confidence\\\": 0.0-1.0}\"},\n",
    "                  {\"role\": \"user\", \"content\": query}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    conf = result.get(\"confidence\", 0.5)\n",
    "    action = ConfidenceAction.AUTO_EXECUTE if conf >= 0.9 else (ConfidenceAction.CONFIRM if conf >= 0.7 else ConfidenceAction.ESCALATE)\n",
    "    return {\"response\": result.get(\"response\"), \"confidence\": conf, \"action\": action}\n",
    "\n",
    "for q in [\"What are your hours?\", \"Should I invest in crypto?\"]:\n",
    "    r = analyze_with_confidence(q)\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    print(f\"  Confidence: {r['confidence']:.0%} â†’ {r['action'].value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Lab 8 Complete!\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Retry with backoff handles transient errors\n",
    "- Fallback chains ensure availability\n",
    "- Circuit breakers prevent cascading failures\n",
    "- Confidence routing protects against uncertainty\n",
    "\n",
    "**Next:** `09_cost_optimization.ipynb`"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}