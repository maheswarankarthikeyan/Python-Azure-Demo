{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš–ï¸ Lab 14: RAG vs Fine-Tuning\n",
    "## Module 14 - Decision Framework for Banking Use Cases\n",
    "\n",
    "**Duration:** 15 minutes\n",
    "\n",
    "**Objectives:**\n",
    "- Compare RAG and fine-tuning approaches\n",
    "- Evaluate for banking scenarios\n",
    "- Implement hybrid approach\n",
    "\n",
    "**Banking Scenario:** Document summarization comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# GOOGLE COLAB SETUP - Add these secrets (click ðŸ”‘ icon):\n",
    "#   - AZURE_OPENAI_KEY: Your API key\n",
    "#   - AZURE_OPENAI_ENDPOINT: https://xxx.openai.azure.com/\n",
    "#   - AZURE_OPENAI_DEPLOYMENT: Your model deployment name\n",
    "# =============================================================================\n",
    "\n",
    "DEMO_MODE = False\n",
    "client = None\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    AZURE_OPENAI_KEY = userdata.get('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_ENDPOINT = userdata.get('AZURE_OPENAI_ENDPOINT')\n",
    "    try:\n",
    "        MODEL_NAME = userdata.get('AZURE_OPENAI_DEPLOYMENT')\n",
    "    except:\n",
    "        pass\n",
    "    if AZURE_OPENAI_KEY and AZURE_OPENAI_ENDPOINT:\n",
    "        if not AZURE_OPENAI_ENDPOINT.startswith('http'):\n",
    "            AZURE_OPENAI_ENDPOINT = 'https://' + AZURE_OPENAI_ENDPOINT\n",
    "        print(f\"âœ… Credentials loaded. Model: {MODEL_NAME}\")\n",
    "    else:\n",
    "        raise ValueError(\"Missing\")\n",
    "except Exception:\n",
    "    print(\"âš ï¸ Running in DEMO MODE\")\n",
    "    DEMO_MODE = True\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    from openai import AzureOpenAI\n",
    "    client = AzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_KEY,\n",
    "        api_version=\"2024-06-01\",\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    "    )\n",
    "    print(\"âœ… Client ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RAG Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated vector store with example summaries\nexample_summaries = [\n    {\"input\": \"Mortgage $300K 30yr 6.5%\", \"output\": \"**Type:** Mortgage\\n**Amount:** $300,000\\n**Term:** 30 years\\n**Rate:** 6.5%\"},\n    {\"input\": \"Auto loan $25K 5yr 7.9%\", \"output\": \"**Type:** Auto\\n**Amount:** $25,000\\n**Term:** 5 years\\n**Rate:** 7.9%\"}\n]\n\n# Use available model (fallback to gpt-4o if gpt-4o-mini not available)\nAVAILABLE_MODEL = MODEL_NAME if MODEL_NAME else \"gpt-4o\"\n\ndef rag_summarize(document: str) -> dict:\n    \"\"\"RAG approach: Retrieve examples, then generate\"\"\"\n    start = time.time()\n    \n    # Simulate retrieval\n    examples_text = \"\\n\".join([f\"Input: {e['input']}\\nOutput: {e['output']}\" for e in example_summaries])\n    \n    # Try gpt-4o-mini first, fallback to available model\n    model_to_use = AVAILABLE_MODEL\n    try:\n        # Test if gpt-4o-mini is available\n        test_response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[{\"role\": \"user\", \"content\": \"test\"}],\n            max_tokens=1\n        )\n        model_to_use = \"gpt-4o-mini\"\n    except:\n        print(\"âš ï¸ gpt-4o-mini not available, using\", AVAILABLE_MODEL)\n        model_to_use = AVAILABLE_MODEL\n    \n    response = client.chat.completions.create(\n        model=model_to_use,\n        messages=[{\n            \"role\": \"system\",\n            \"content\": f\"Summarize loan documents like these examples:\\n{examples_text}\"\n        }, {\n            \"role\": \"user\",\n            \"content\": f\"Summarize: {document}\"\n        }]\n    )\n    \n    return {\n        \"output\": response.choices[0].message.content,\n        \"latency\": time.time() - start,\n        \"tokens\": response.usage.total_tokens,\n        \"approach\": f\"RAG ({model_to_use})\",\n        \"model_used\": model_to_use\n    }\n\nprint(f\"âœ… RAG function defined (will use {AVAILABLE_MODEL} if gpt-4o-mini unavailable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fine-Tuned Approach (Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuned_summarize(document: str) -> dict:\n    \"\"\"Fine-tuned approach: Direct generation\"\"\"\n    start = time.time()\n    \n    # In production: use fine-tuned model\n    # model = \"ft:gpt-4o-mini:banking:loan-summarizer\"\n    \n    # Try gpt-4o-mini first, fallback to available model\n    model_to_use = AVAILABLE_MODEL\n    try:\n        # Test if gpt-4o-mini is available\n        test_response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[{\"role\": \"user\", \"content\": \"test\"}],\n            max_tokens=1\n        )\n        model_to_use = \"gpt-4o-mini\"\n    except:\n        model_to_use = AVAILABLE_MODEL\n    \n    response = client.chat.completions.create(\n        model=model_to_use,\n        messages=[{\n            \"role\": \"system\",\n            \"content\": \"Summarize loan documents in this format: **Type:** X\\n**Amount:** $X\\n**Term:** X years\\n**Rate:** X%\"\n        }, {\n            \"role\": \"user\",\n            \"content\": document\n        }]\n    )\n    \n    return {\n        \"output\": response.choices[0].message.content,\n        \"latency\": time.time() - start,\n        \"tokens\": response.usage.total_tokens,\n        \"approach\": f\"Fine-tuned ({model_to_use})\",\n        \"model_used\": model_to_use\n    }\n\nprint(f\"âœ… Fine-tuned function defined (will use {AVAILABLE_MODEL} if gpt-4o-mini unavailable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Compare Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = \"Home equity line of credit for $100,000 at 9.25% variable rate for 10 years. Applicant has $250,000 home equity.\"\n\nprint(\"Testing both approaches...\\n\")\n\ntry:\n    rag_result = rag_summarize(test_doc)\n    ft_result = finetuned_summarize(test_doc)\n    \n    print(\"=\"*60)\n    print(\"COMPARISON RESULTS\")\n    print(\"=\"*60)\n    print(f\"\\n{'Metric':<15} {'RAG':<25} {'Fine-tuned':<25}\")\n    print(\"-\"*70)\n    print(f\"{'Model Used':<15} {rag_result.get('model_used', 'N/A'):<25} {ft_result.get('model_used', 'N/A'):<25}\")\n    print(f\"{'Latency':<15} {rag_result['latency']:.2f}s{'':<19} {ft_result['latency']:.2f}s\")\n    print(f\"{'Tokens':<15} {rag_result['tokens']:<25} {ft_result['tokens']:<25}\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"RAG Output:\")\n    print(rag_result['output'])\n    print(\"\\n\" + \"=\"*60)\n    print(\"Fine-tuned Output:\")\n    print(ft_result['output'])\n    \nexcept Exception as e:\n    print(f\"âŒ Error running comparison: {e}\")\n    print(\"\\nðŸ’¡ Make sure you have deployed at least one model (gpt-4o or gpt-4o-mini)\")\n    print(\"\\nðŸ”§ To deploy gpt-4o-mini:\")\n    print(\"   az cognitiveservices account deployment create \\\\\")\n    print(\"     --name banking-openai-service \\\\\")\n    print(\"     --resource-group rg-banking-ai \\\\\")\n    print(\"     --deployment-name gpt-4o-mini \\\\\")\n    print(\"     --model-name gpt-4o-mini \\\\\")\n    print(\"     --model-version '2024-07-18' \\\\\")\n    print(\"     --model-format OpenAI \\\\\")\n    print(\"     --sku-capacity 10 \\\\\")\n    print(\"     --sku-name Standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Decision Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_approach(use_case: dict) -> str:\n    \"\"\"Recommend RAG, Fine-tuning, or Hybrid\"\"\"\n    \n    knowledge_changes = use_case.get(\"knowledge_changes_frequently\", False)\n    needs_format = use_case.get(\"needs_consistent_format\", False)\n    needs_sources = use_case.get(\"needs_source_citations\", False)\n    latency_critical = use_case.get(\"latency_critical\", False)\n    \n    if knowledge_changes or needs_sources:\n        if needs_format:\n            return \"HYBRID: RAG for knowledge + Fine-tuned for format\"\n        return \"RAG: Knowledge changes frequently or needs citations\"\n    \n    if needs_format or latency_critical:\n        return \"FINE-TUNE: Consistent format or low latency needed\"\n    \n    return \"PROMPT ENGINEERING: Start simple, iterate\"\n\n# Test recommendations\nuse_cases = [\n    {\"name\": \"Policy FAQ Bot\", \"knowledge_changes_frequently\": True, \"needs_source_citations\": True},\n    {\"name\": \"Loan Classifier\", \"needs_consistent_format\": True, \"latency_critical\": True},\n    {\"name\": \"Document Processor\", \"knowledge_changes_frequently\": True, \"needs_consistent_format\": True}\n]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RECOMMENDATIONS\")\nprint(\"=\"*60)\nfor uc in use_cases:\n    rec = recommend_approach(uc)\n    print(f\"\\n{uc['name']}:\")\n    print(f\"  â†’ {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Lab 14 Complete!\n",
    "\n",
    "**Key Takeaways:**\n",
    "- RAG: Best for changing knowledge, source citations\n",
    "- Fine-tuning: Best for consistent format, low latency\n",
    "- Hybrid: Combine both for complex use cases\n",
    "- Always start with prompt engineering first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
